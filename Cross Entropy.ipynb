{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning:\n",
    "\n",
    "Cross Entropy Method on : CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode is used to store discounted reward and a collection of episode step.\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "# EpisodeStep is used to store info about a single step - action and observation from env in an episode. We will use steps\n",
    "# From Elite Episodes as training data.\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = [] # Store Episode Named Tuple.\n",
    "    episode_reward = 0.0 # Stores epiode reward.\n",
    "    episode_steps = [] # Stored EpisodeStep Named tuple.\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1) # Apply softmax to output to get probabilities.\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs]) # Convert the current observation to PyTorch Tensor of size 1*4 ([obs])\n",
    "        act_probs_v = sm(net(obs_v)) # Get Probabilities by passing thru NN and then applying softmax.\n",
    "        act_probs = act_probs_v.data.numpy()[0] # Convert to Numpy and get probabilities.\n",
    "        action = np.random.choice(len(act_probs), p=act_probs) # Randomly sample probability distributed actions to get a action\n",
    "        next_obs, reward, is_done, _ = env.step(action) # Perform the action.\n",
    "        episode_reward += reward # Accumulate the reward for the current episode.\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action)) # Store the observation and action taken in list.\n",
    "        if is_done: # If episode Terminates.\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps)) # Store Discounted Reward and obs and step pair.\n",
    "            episode_reward = 0.0 # DO this 0 for next episode.\n",
    "            episode_steps = [] # Empty for next episode.\n",
    "            next_obs = env.reset() # Reset the environment.\n",
    "            if len(batch) == batch_size: # If number of episodes = batch size to be passed in NN then.\n",
    "                yield batch # Return Batch (Generator Function).\n",
    "                batch = [] # Empty for next batch.\n",
    "        obs = next_obs # if episode done then env.reset() else next state is given to obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch)) # Get rewards from batch.\n",
    "    reward_bound = np.percentile(rewards, percentile) # Calculate the percentile value.\n",
    "    reward_mean = float(np.mean(rewards)) # Calculate the mean of the percentile.\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound: # If reward lesser than coundary reward then do nothing.\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps)) # Store observation.\n",
    "        train_act.extend(map(lambda step: step.action, example.steps)) # Store action corresponding to given observation.\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs) # Convert to Tensor.\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.695, reward_mean=21.8, reward_bound=25.0\n",
      "1: loss=0.698, reward_mean=20.3, reward_bound=22.0\n",
      "2: loss=0.685, reward_mean=19.4, reward_bound=21.5\n",
      "3: loss=0.674, reward_mean=26.6, reward_bound=27.5\n",
      "4: loss=0.666, reward_mean=28.2, reward_bound=32.0\n",
      "5: loss=0.647, reward_mean=35.9, reward_bound=37.0\n",
      "6: loss=0.630, reward_mean=35.9, reward_bound=41.5\n",
      "7: loss=0.659, reward_mean=40.7, reward_bound=46.5\n",
      "8: loss=0.639, reward_mean=49.6, reward_bound=62.0\n",
      "9: loss=0.604, reward_mean=51.9, reward_bound=54.0\n",
      "10: loss=0.594, reward_mean=59.3, reward_bound=59.0\n",
      "11: loss=0.599, reward_mean=53.2, reward_bound=58.0\n",
      "12: loss=0.605, reward_mean=49.1, reward_bound=60.5\n",
      "13: loss=0.631, reward_mean=55.4, reward_bound=61.5\n",
      "14: loss=0.585, reward_mean=61.6, reward_bound=64.5\n",
      "15: loss=0.574, reward_mean=61.6, reward_bound=78.5\n",
      "16: loss=0.572, reward_mean=54.1, reward_bound=64.0\n",
      "17: loss=0.549, reward_mean=53.6, reward_bound=64.5\n",
      "18: loss=0.575, reward_mean=59.8, reward_bound=69.0\n",
      "19: loss=0.560, reward_mean=66.4, reward_bound=76.5\n",
      "20: loss=0.558, reward_mean=80.5, reward_bound=103.0\n",
      "21: loss=0.539, reward_mean=80.2, reward_bound=83.5\n",
      "22: loss=0.554, reward_mean=62.2, reward_bound=80.5\n",
      "23: loss=0.564, reward_mean=73.5, reward_bound=83.5\n",
      "24: loss=0.543, reward_mean=78.2, reward_bound=80.5\n",
      "25: loss=0.532, reward_mean=70.7, reward_bound=79.0\n",
      "26: loss=0.545, reward_mean=81.6, reward_bound=81.5\n",
      "27: loss=0.525, reward_mean=84.5, reward_bound=95.0\n",
      "28: loss=0.557, reward_mean=98.4, reward_bound=109.0\n",
      "29: loss=0.522, reward_mean=101.4, reward_bound=120.0\n",
      "30: loss=0.534, reward_mean=108.8, reward_bound=121.5\n",
      "31: loss=0.553, reward_mean=134.9, reward_bound=151.5\n",
      "32: loss=0.541, reward_mean=140.2, reward_bound=161.0\n",
      "33: loss=0.539, reward_mean=157.9, reward_bound=190.5\n",
      "34: loss=0.542, reward_mean=169.7, reward_bound=200.0\n",
      "35: loss=0.541, reward_mean=177.3, reward_bound=200.0\n",
      "36: loss=0.535, reward_mean=173.8, reward_bound=200.0\n",
      "37: loss=0.531, reward_mean=197.2, reward_bound=200.0\n",
      "38: loss=0.535, reward_mean=190.9, reward_bound=200.0\n",
      "39: loss=0.549, reward_mean=191.4, reward_bound=200.0\n",
      "40: loss=0.537, reward_mean=193.5, reward_bound=200.0\n",
      "41: loss=0.544, reward_mean=197.7, reward_bound=200.0\n",
      "42: loss=0.539, reward_mean=186.6, reward_bound=200.0\n",
      "43: loss=0.535, reward_mean=189.1, reward_bound=200.0\n",
      "44: loss=0.543, reward_mean=188.3, reward_bound=200.0\n",
      "45: loss=0.542, reward_mean=180.7, reward_bound=200.0\n",
      "46: loss=0.546, reward_mean=194.9, reward_bound=200.0\n",
      "47: loss=0.538, reward_mean=193.5, reward_bound=200.0\n",
      "48: loss=0.539, reward_mean=183.1, reward_bound=200.0\n",
      "49: loss=0.541, reward_mean=182.9, reward_bound=200.0\n",
      "50: loss=0.539, reward_mean=198.1, reward_bound=200.0\n",
      "51: loss=0.533, reward_mean=194.9, reward_bound=200.0\n",
      "52: loss=0.541, reward_mean=191.9, reward_bound=200.0\n",
      "53: loss=0.527, reward_mean=200.0, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\") # Cartpole Env\n",
    "    env = gym.wrappers.Monitor(env, directory=\"cross_entropy_CartPole\", force=True) # Summary for TensorBoardX.\n",
    "    obs_size = env.observation_space.shape[0] # All Observations, known environment.\n",
    "    n_actions = env.action_space.n # All Actions.\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions) # Create the model.\n",
    "    objective = nn.CrossEntropyLoss() # Minimize the loss.\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "    # Generate bataches i.e a list of 16 episodes.\n",
    "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)): # Continue until reward is greater than 199.\n",
    "        obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE) # Filter batches generated.\n",
    "        optimizer.zero_grad() # 0 the previous gradient values.\n",
    "        action_scores_v = net(obs_v) # Calculate the action values.\n",
    "        loss_v = objective(action_scores_v, acts_v) # Calculate the CrossEntropyLoss.\n",
    "        loss_v.backward() # Caculate gradients.\n",
    "        optimizer.step() # apply changes to NN weights.\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "        writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "        writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break\n",
    "    writer.close()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
