{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning:\n",
    "\n",
    "CartPole Environment: https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Solution using:\n",
    "\n",
    "1. Random Search to find policy.\n",
    "2. Using Hill Climbing to find policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('r1')\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to run episodes\n",
    "class Harness:\n",
    "    def run_episode(self, env, agent):\n",
    "        observation = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(1000):\n",
    "            action = agent.next_action(observation)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for Agent.\n",
    "class LinearAgent:\n",
    "    \n",
    "    # Initialize Parameters\n",
    "    def __init__(self):\n",
    "        self.parameters = np.random.rand(4) * 2 - 1\n",
    "    \n",
    "    # Get action 0 or 1\n",
    "    def next_action(self, observation):\n",
    "        return 0 if np.matmul(self.parameters, observation) < 0 else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    best_parameters = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "    \n",
    "    # Training\n",
    "    for step in range(1000000):\n",
    "        agent.parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "            best_parameters = agent.parameters\n",
    "            \n",
    "        if step % 5 == 0:\n",
    "            print(reward)\n",
    "            print(agent.parameters)\n",
    "        \n",
    "        if reward == 200:\n",
    "            break\n",
    "    \n",
    "    # Display performance on optimal policy found.\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(agent.next_action(obs))\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200.0\n",
      "[-0.58457888  0.99539068 -0.03681355  0.90831454]\n"
     ]
    }
   ],
   "source": [
    "random_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hill Climbing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hill_climbing():\n",
    "    env = gym.make('CartPole-v0')\n",
    "    best_parameters = None\n",
    "    best_reward = 0\n",
    "    agent = LinearAgent()\n",
    "    harness = Harness()\n",
    "    noise_scaling = 0.1\n",
    "    \n",
    "    for step in range(10000):\n",
    "        old_params = agent.parameters\n",
    "        agent.parameters += noise_scaling * (np.random.rand(4) * 2 - 1) \n",
    "        reward = harness.run_episode(env, agent)\n",
    "        if reward > best_reward:\n",
    "            best_reward = reward\n",
    "        else:\n",
    "            agent.parameters = old_params\n",
    "            \n",
    "        if step % 100 == 0 or reward == 200:\n",
    "            print(reward)\n",
    "        \n",
    "        if reward == 200:\n",
    "            break\n",
    "            \n",
    "    \n",
    "    # Display performance on optimal policy found.\n",
    "    obs = env.reset()\n",
    "    for _ in range(1000):\n",
    "        env.render()\n",
    "        obs, reward, done, info = env.step(agent.next_action(obs))\n",
    "        if done:\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0\n",
      "96.0\n",
      "72.0\n",
      "164.0\n",
      "117.0\n",
      "130.0\n",
      "106.0\n",
      "104.0\n",
      "110.0\n",
      "92.0\n",
      "60.0\n",
      "90.0\n",
      "76.0\n",
      "127.0\n",
      "144.0\n",
      "130.0\n",
      "177.0\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "hill_climbing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
