{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning:\n",
    "\n",
    "Contextual Bandit.\n",
    "> Has multiple bandist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for multiple bandits:\n",
    "class ContextualBandit:\n",
    "    def __init__(self):\n",
    "        self.active_bandit = 0 # state\n",
    "        self.bandits = np.array([\n",
    "            [0.2, 0.0, 0.1, -4.0], # 4th arm is best\n",
    "            [0.1, -5.0, 1.0, 0.25], # 2nd arm is best\n",
    "            [-3.5, 2.0, 3.2, 6.4] # 1st arm is best\n",
    "        ])\n",
    "        self.num_bandits, self.num_actions = self.bandits.shape\n",
    "        \n",
    "    def get_bandit(self):\n",
    "        self.active_bandit = np.random.randint(0, self.num_bandits)\n",
    "        return self.active_bandit\n",
    "    \n",
    "    def pull(self, arm):\n",
    "        bandit = self.bandits[self.active_bandit, arm]\n",
    "        return 1 if np.random.randn(1) > bandit else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy gradient based RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, learning_rate = 1e-3, contexts = 3, actions = 4):\n",
    "        self.num_actions = actions\n",
    "        self.reward_in = tf.placeholder(tf.float32, [1], name='reward_in')\n",
    "        self.context_in = tf.placeholder(tf.int32, [1], name='context_in')\n",
    "        self.action_in = tf.placeholder(tf.int32, [1], name='reward_in')\n",
    "        \n",
    "        # sess.run(best_action)\n",
    "        context_one_hot = tf.one_hot(self.context_in, contexts)\n",
    "        W = tf.get_variable('W', [contexts, actions])\n",
    "        self.output = tf.nn.sigmoid(tf.matmul(context_one_hot, W))\n",
    "        self.best_action = tf.argmax(self.output, axis = 1)\n",
    "        \n",
    "        # sess.run(optimizer)\n",
    "        a_ = tf.reduce_mean(self.output * tf.one_hot(self.action_in, actions))\n",
    "        self.loss = -(tf.log(a_) * self.reward_in)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def predict(self, sess, context):\n",
    "        return sess.run(self.best_action, {self.context_in: [context]})[0]\n",
    "    \n",
    "    def random_or_predict(self, sess, epsilon, context):\n",
    "        if np.random.randn(1) < epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return self.predict(sess, context)\n",
    "        \n",
    "    def train(self, sess, context, action, reward):\n",
    "        sess.run(self.optimizer, {\n",
    "            self.action_in: [action],\n",
    "            self.reward_in: [reward],\n",
    "            self.context_in: [context]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\program files\\python36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "env = ContextualBandit()\n",
    "agent = Agent()\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current State: 1\n",
      "Prediction: action 2 for state 1\n",
      "Reward: 1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    context = env.get_bandit()\n",
    "    print('Current State: {}'.format(context))\n",
    "    action = agent.random_or_predict(sess, epsilon, context)\n",
    "    print('Prediction: action {} for state {}'.format(action, context))\n",
    "    reward = env.pull(action)\n",
    "    print('Reward: {}'.format(reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Loss: [-1.8021215]\n",
      "Step: 500, Loss: [-1.804075]\n",
      "Step: 1000, Loss: [-1.8045963]\n",
      "Step: 1500, Loss: [-2.5746338]\n",
      "Step: 2000, Loss: [-2.0856092]\n",
      "Step: 2500, Loss: [-1.8613781]\n",
      "Step: 3000, Loss: [-1.8849858]\n",
      "Step: 3500, Loss: [2.1292763]\n",
      "Step: 4000, Loss: [-1.9118143]\n",
      "Step: 4500, Loss: [-2.6657019]\n",
      "Step: 5000, Loss: [1.4790648]\n",
      "Step: 5500, Loss: [2.579819]\n",
      "Step: 6000, Loss: [1.5114468]\n",
      "Step: 6500, Loss: [1.4439652]\n",
      "Step: 7000, Loss: [1.6446321]\n",
      "Step: 7500, Loss: [1.608673]\n",
      "Step: 8000, Loss: [-2.5699105]\n",
      "Step: 8500, Loss: [2.0608175]\n",
      "Step: 9000, Loss: [1.4142538]\n",
      "Step: 9500, Loss: [1.5065866]\n",
      "Step: 10000, Loss: [1.4071814]\n",
      "Step: 10500, Loss: [-3.2510664]\n",
      "Step: 11000, Loss: [-3.4068289]\n",
      "Step: 11500, Loss: [1.4559808]\n",
      "Step: 12000, Loss: [1.4471116]\n",
      "Step: 12500, Loss: [1.3962195]\n",
      "Step: 13000, Loss: [1.4016391]\n",
      "Step: 13500, Loss: [-1.9115477]\n",
      "Step: 14000, Loss: [1.3929719]\n",
      "Step: 14500, Loss: [1.4142817]\n",
      "Step: 15000, Loss: [1.3951322]\n",
      "Step: 15500, Loss: [-4.313585]\n",
      "Step: 16000, Loss: [1.4041036]\n",
      "Step: 16500, Loss: [1.4018354]\n",
      "Step: 17000, Loss: [1.389004]\n",
      "Step: 17500, Loss: [-3.3456182]\n",
      "Step: 18000, Loss: [-5.2662225]\n",
      "Step: 18500, Loss: [1.3895159]\n",
      "Step: 19000, Loss: [1.3939282]\n",
      "Step: 19500, Loss: [-3.5963278]\n",
      "Step: 20000, Loss: [2.7858942]\n",
      "Step: 20500, Loss: [1.3872553]\n",
      "Step: 21000, Loss: [1.3906238]\n",
      "Step: 21500, Loss: [1.3870237]\n",
      "Step: 22000, Loss: [1.3894783]\n",
      "Step: 22500, Loss: [2.7543232]\n",
      "Step: 23000, Loss: [-4.086281]\n",
      "Step: 23500, Loss: [-4.1442227]\n",
      "Step: 24000, Loss: [1.3869308]\n",
      "Step: 24500, Loss: [1.3866049]\n",
      "Step: 25000, Loss: [1.3867706]\n",
      "Step: 25500, Loss: [-6.790436]\n",
      "Step: 26000, Loss: [1.3872907]\n",
      "Step: 26500, Loss: [1.3866038]\n",
      "Step: 27000, Loss: [1.387034]\n",
      "Step: 27500, Loss: [2.0209]\n",
      "Step: 28000, Loss: [1.3864908]\n",
      "Step: 28500, Loss: [1.3864666]\n",
      "Step: 29000, Loss: [1.386705]\n",
      "Step: 29500, Loss: [1.386369]\n",
      "Step: 30000, Loss: [1.3864064]\n",
      "Step: 30500, Loss: [-2.64274]\n",
      "Step: 31000, Loss: [1.386521]\n",
      "Step: 31500, Loss: [1.3863672]\n",
      "Step: 32000, Loss: [1.3863299]\n",
      "Step: 32500, Loss: [2.6933117]\n",
      "Step: 33000, Loss: [1.3863212]\n",
      "Step: 33500, Loss: [-5.4333034]\n",
      "Step: 34000, Loss: [2.060527]\n",
      "Step: 34500, Loss: [-2.8492873]\n",
      "Step: 35000, Loss: [-8.225749]\n",
      "Step: 35500, Loss: [1.3863177]\n",
      "Step: 36000, Loss: [-8.971954]\n",
      "Step: 36500, Loss: [1.3863041]\n",
      "Step: 37000, Loss: [2.8146718]\n",
      "Step: 37500, Loss: [-8.749388]\n",
      "Step: 38000, Loss: [1.3863252]\n",
      "Step: 38500, Loss: [1.3863212]\n",
      "Step: 39000, Loss: [1.3862991]\n",
      "Step: 39500, Loss: [-6.2966385]\n",
      "Step: 40000, Loss: [-2.1547163]\n",
      "Step: 40500, Loss: [1.3862975]\n",
      "Step: 41000, Loss: [-9.573351]\n",
      "Step: 41500, Loss: [1.3862984]\n",
      "Step: 42000, Loss: [1.3862978]\n",
      "Step: 42500, Loss: [1.386296]\n",
      "Step: 43000, Loss: [1.3862958]\n",
      "Step: 43500, Loss: [1.3863007]\n",
      "Step: 44000, Loss: [-2.1963146]\n",
      "Step: 44500, Loss: [1.3862991]\n",
      "Step: 45000, Loss: [1.3862952]\n",
      "Step: 45500, Loss: [-11.035596]\n",
      "Step: 46000, Loss: [1.3862956]\n",
      "Step: 46500, Loss: [1.386295]\n",
      "Step: 47000, Loss: [1.3862948]\n",
      "Step: 47500, Loss: [-11.448444]\n",
      "Step: 48000, Loss: [-7.4385467]\n",
      "Step: 48500, Loss: [1.3862947]\n",
      "Step: 49000, Loss: [1.3862946]\n",
      "Step: 49500, Loss: [-7.6321893]\n"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "num_episodes = 50000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for ep in range(num_episodes):\n",
    "        context = env.get_bandit()\n",
    "        action = agent.random_or_predict(sess, epsilon, context)\n",
    "        reward = env.pull(action)\n",
    "        # Feed back to train\n",
    "        agent.train(sess, context, action, reward)\n",
    "        if ep % 500 == 0:\n",
    "            loss = sess.run(agent.loss, {\n",
    "                agent.action_in: [action],\n",
    "                agent.reward_in: [reward],\n",
    "                agent.context_in: [context]\n",
    "            })\n",
    "            print('Step: {}, Loss: {}'.format(ep, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
